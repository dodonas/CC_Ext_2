{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLOUD_TECH_EXT_2_PRODUCER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwxw+6wvE7rJ3OmgbRjxoI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dodonas/CC_Ext_2/blob/main/_notebooks/CLOUD_TECH_EXT_2_PRODUCER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "sDHGFU9wPNux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8Y8pnh0v0Th"
      },
      "outputs": [],
      "source": [
        "!pip install confluent_kafka\n",
        "!pip install kafka-python\n",
        "# --------- Spark -----------\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget https://downloads.apache.org/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!tar -xvf spark-2.4.8-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!wget \"https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.4.8/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and sets"
      ],
      "metadata": {
        "id": "MWBVUI5_4AiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer\n",
        "from itertools import islice\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime\n",
        "import threading\n",
        "from kafka import KafkaProducer\n",
        "from kafka.errors import KafkaError"
      ],
      "metadata": {
        "id": "6-xJIhHP39-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.8-bin-hadoop2.7\"\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /content/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
      ],
      "metadata": {
        "id": "ZSQSKNsMPq4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "isf8g8foSVzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Normalizer, StandardScaler\n",
        "import random\n",
        "import pyspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.streaming.kafka import KafkaUtils\n",
        "from uuid import uuid1"
      ],
      "metadata": {
        "id": "kbuqingHTA_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables"
      ],
      "metadata": {
        "id": "Jc6_jRzBTQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kafka_topic_name = \"RomeoAndJuliet\"\n",
        "kafka_bootstrap_servers = 'localhost:9092'"
      ],
      "metadata": {
        "id": "0q3lcto9TW0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "SbwfPLAXTu74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_time():\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H:%M:%S\")\n",
        "  print(\"Current Time =\", current_time)"
      ],
      "metadata": {
        "id": "-3F3ros5TxMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def connect_kafka_producer(server):\n",
        "    _producer = None\n",
        "    try:\n",
        "        _producer = Producer({'bootstrap.servers': server})\n",
        "        print('Connected {}'.format(_producer))\n",
        "    except Exception as ex:\n",
        "        print('Exception while connecting Kafka: {}'.format(str(ex)))\n",
        "    finally:\n",
        "        return _producer"
      ],
      "metadata": {
        "id": "0Eo3FJvaD97r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function is called once for each message generated to indicate the delivery result"
      ],
      "metadata": {
        "id": "6wDWlTN9wLHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delivery_callback(err, msg):\n",
        "  if err:\n",
        "    sys.stderr.write('%% Message failed delivery: %s\\n' % err)\n",
        "  else:\n",
        "    sys.stderr.write('%% Message delivered to %s [%d] @ %d\\n' % (msg.topic(), msg.partition(), msg.offset()))"
      ],
      "metadata": {
        "id": "jr_SVc0Ev8XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each time the function below is called it will return the next 100 (or less) lines."
      ],
      "metadata": {
        "id": "vCDDeOHG0vCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunker(seq, size):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
      ],
      "metadata": {
        "id": "uBPU7Sg_07F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def produce():\n",
        "  p = connect_kafka_producer('localhost:9092')\n",
        "  with open(\"RomeoAndJuliet.txt\") as fp:\n",
        "    lines = fp.readlines()\n",
        "    for chunk in chunker(lines, 100):\n",
        "        for line in chunk:\n",
        "          try:\n",
        "            p.produce('RomeoAndJuliet', line.rstrip(), callback=delivery_callback)\n",
        "          except BufferError:\n",
        "            sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' % len(p))\n",
        "          p.poll(0)\n",
        "        time.sleep(2)\n",
        "  p.flush()"
      ],
      "metadata": {
        "id": "59MhQMjGw4OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "produce()"
      ],
      "metadata": {
        "id": "WVgbbfIRFVl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}